"""
Stage 4: Text Extraction API Endpoints
RESTful API for extracting and managing text from creative documents
Part of knowNothing Creative RAG
"""

from fastapi import APIRouter, HTTPException, Query
from fastapi.responses import JSONResponse
from typing import Dict, Any, Optional
import logging
from datetime import datetime

from ..services.text_extractor import TextExtractor

logger = logging.getLogger(__name__)

# Create router with prefix
router = APIRouter(prefix="/api/text", tags=["Text Extraction"])

# Initialize text extractor
text_extractor = TextExtractor()

@router.post("/extract/{document_id}")
async def extract_text_from_document(document_id: str) -> Dict[str, Any]:
    """
    Extract text from an uploaded document
    
    - **document_id**: ID of document to extract text from
    
    Perfect for getting readable text from scripts, notes, and research documents!
    """
    logger.info(f"ğŸ“„ Extracting text from document: {document_id}")
    
    try:
        # Check if text already extracted
        existing_text = await text_extractor.get_extracted_text(document_id)
        if existing_text:
            return {
                "success": True,
                "message": "ğŸ“„ Text already extracted (returning cached version)",
                "document_id": document_id,
                "extraction_method": existing_text["method"],
                "text_preview": existing_text["text"][:500] + "..." if len(existing_text["text"]) > 500 else existing_text["text"],
                "statistics": {
                    "word_count": existing_text["word_count"],
                    "character_count": existing_text["character_count"],
                    "page_count": existing_text["page_count"]
                },
                "extraction_date": existing_text["extraction_date"],
                "cached": True,
                "timestamp": datetime.utcnow().isoformat()
            }
        
        # Extract text
        result = await text_extractor.extract_text_from_document(document_id)
        
        if result["success"]:
            return {
                "success": True,
                "message": "ğŸ“„ Text extracted successfully!",
                **result,
                "cached": False,
                "next_steps": [
                    f"Get full text: GET /api/text/{document_id}",
                    f"Search within text: POST /api/text/{document_id}/search",
                    "Get extraction statistics: GET /api/text/stats"
                ]
            }
        else:
            raise HTTPException(
                status_code=422,
                detail=f"ğŸ”§ Text extraction failed: {result.get('error', 'Unknown error')}"
            )
            
    except HTTPException:
        # Re-raise HTTP exceptions
        raise
        
    except Exception as e:
        logger.error(f"âŒ Text extraction failed for {document_id}: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ğŸ”§ Text extraction failed: {str(e)}"
        )

@router.get("/{document_id}")
async def get_extracted_text(
    document_id: str,
    format: str = Query(default="preview", regex="^(preview|full|metadata)$", description="Text format to return")
) -> Dict[str, Any]:
    """
    Get extracted text for a document
    
    - **document_id**: ID of document
    - **format**: Return format (preview/full/metadata)
    
    Access the full text content of your creative documents!
    """
    logger.info(f"ğŸ“– Getting extracted text for document: {document_id} (format: {format})")
    
    try:
        text_data = await text_extractor.get_extracted_text(document_id)
        
        if not text_data:
            raise HTTPException(
                status_code=404,
                detail=f"ğŸ“„ No extracted text found for document '{document_id}'. Try extracting it first!"
            )
        
        if format == "metadata":
            return {
                "success": True,
                "message": "ğŸ“Š Text metadata retrieved",
                "document_id": document_id,
                "metadata": {
                    "extraction_method": text_data["method"],
                    "word_count": text_data["word_count"],
                    "character_count": text_data["character_count"],
                    "page_count": text_data["page_count"],
                    "extraction_date": text_data["extraction_date"],
                    "processing_notes": text_data["processing_notes"]
                },
                "timestamp": datetime.utcnow().isoformat()
            }
        
        elif format == "preview":
            preview_text = text_data["text"][:1000] + "..." if len(text_data["text"]) > 1000 else text_data["text"]
            return {
                "success": True,
                "message": "ğŸ“– Text preview retrieved",
                "document_id": document_id,
                "text_preview": preview_text,
                "is_truncated": len(text_data["text"]) > 1000,
                "full_length": len(text_data["text"]),
                "statistics": {
                    "word_count": text_data["word_count"],
                    "character_count": text_data["character_count"],
                    "extraction_method": text_data["method"]
                },
                "timestamp": datetime.utcnow().isoformat()
            }
        
        else:  # format == "full"
            return {
                "success": True,
                "message": "ğŸ“– Full text retrieved",
                "document_id": document_id,
                "extracted_text": text_data["text"],
                "statistics": {
                    "word_count": text_data["word_count"],
                    "character_count": text_data["character_count"],
                    "page_count": text_data["page_count"],
                    "extraction_method": text_data["method"],
                    "extraction_date": text_data["extraction_date"]
                },
                "processing_notes": text_data["processing_notes"],
                "timestamp": datetime.utcnow().isoformat()
            }
            
    except HTTPException:
        # Re-raise HTTP exceptions
        raise
        
    except Exception as e:
        logger.error(f"âŒ Failed to get extracted text: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ğŸ”§ Couldn't get extracted text: {str(e)}"
        )

@router.post("/{document_id}/search")
async def search_in_document(
    document_id: str,
    query: str = Query(..., description="Text to search for"),
    case_sensitive: bool = Query(default=False, description="Case-sensitive search"),
    whole_words: bool = Query(default=False, description="Match whole words only")
) -> Dict[str, Any]:
    """
    Search for text within a document
    
    - **document_id**: ID of document to search
    - **query**: Text to search for
    - **case_sensitive**: Whether search should be case-sensitive
    - **whole_words**: Whether to match whole words only
    
    Find specific content in your creative documents!
    """
    logger.info(f"ğŸ” Searching in document {document_id} for: '{query}'")
    
    try:
        text_data = await text_extractor.get_extracted_text(document_id)
        
        if not text_data:
            raise HTTPException(
                status_code=404,
                detail=f"ğŸ“„ Document text not found. Extract text first!"
            )
        
        # Perform search
        text = text_data["text"]
        search_text = text if case_sensitive else text.lower()
        search_query = query if case_sensitive else query.lower()
        
        # Find matches
        matches = []
        start = 0
        
        while True:
            if whole_words:
                import re
                pattern = r'\b' + re.escape(search_query) + r'\b'
                match = re.search(pattern, search_text[start:])
                if not match:
                    break
                pos = start + match.start()
            else:
                pos = search_text.find(search_query, start)
                if pos == -1:
                    break
            
            # Get context around match
            context_start = max(0, pos - 100)
            context_end = min(len(text), pos + len(query) + 100)
            context = text[context_start:context_end]
            
            matches.append({
                "position": pos,
                "context": context,
                "line_number": text[:pos].count('\n') + 1
            })
            
            start = pos + 1
        
        return {
            "success": True,
            "message": f"ğŸ” Found {len(matches)} matches for '{query}'",
            "document_id": document_id,
            "query": query,
            "matches": matches[:50],  # Limit to 50 matches
            "total_matches": len(matches),
            "search_options": {
                "case_sensitive": case_sensitive,
                "whole_words": whole_words
            },
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except HTTPException:
        # Re-raise HTTP exceptions
        raise
        
    except Exception as e:
        logger.error(f"âŒ Search failed: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ğŸ”§ Search failed: {str(e)}"
        )

@router.get("/stats")
async def get_text_extraction_stats() -> Dict[str, Any]:
    """
    Get text extraction statistics
    
    Shows extraction progress and system capabilities!
    """
    logger.info("ğŸ“Š Getting text extraction statistics")
    
    try:
        stats = await text_extractor.get_text_extraction_status()
        
        return {
            "success": True,
            "message": "ğŸ“Š Text extraction statistics retrieved",
            "statistics": stats,
            "recommendations": [
                "PDF files work best with clear, text-based content",
                "DOCX files provide the most accurate extraction",
                "TXT files are processed instantly with perfect accuracy"
            ] if stats.get("total_documents", 0) > 0 else [
                "Upload documents to see extraction statistics",
                "Supported formats: PDF, DOCX, TXT",
                "Text extraction enables search and AI analysis"
            ],
            "capabilities": {
                "formats_supported": ["TXT", "PDF (if PyPDF2 installed)", "DOCX (if python-docx installed)"],
                "features": [
                    "Automatic text cleaning and normalization",
                    "Word and character counting",
                    "Page counting for PDFs",
                    "Search within extracted text",
                    "Multiple encoding support for text files"
                ]
            },
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ Failed to get extraction stats: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ğŸ”§ Couldn't get extraction stats: {str(e)}"
        )

@router.post("/extract-all")
async def extract_all_documents() -> Dict[str, Any]:
    """
    Extract text from all documents that don't have text extracted yet
    
    Batch process all your creative documents!
    """
    logger.info("ğŸ“„ Starting batch text extraction")
    
    try:
        # This would be implemented to process all unextracted documents
        # For now, return a helpful message
        return {
            "success": True,
            "message": "ğŸš§ Batch extraction feature coming soon!",
            "recommendation": "For now, extract text from individual documents using POST /api/text/extract/{document_id}",
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ Batch extraction failed: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ğŸ”§ Batch extraction failed: {str(e)}"
        )